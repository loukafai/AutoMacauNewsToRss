import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import re
import datetime
import email.utils
import xml.sax.saxutils as saxutils
from concurrent.futures import ThreadPoolExecutor, as_completed
import sys
import os

def fetch_single_article(i, link, headers):
    """å–®ç¯‡æ–‡ç« æŠ“å–é‚è¼¯"""
    try:
        r = requests.get(link, headers=headers, timeout=15)
        r.encoding = 'utf-8'
        raw_html = r.text

        title_match = re.search(r'<founder-title>(.*?)</founder-title>', raw_html, re.DOTALL)
        final_title = title_match.group(1).strip() if title_match else "ç„¡æ¨™é¡Œ"
        final_title = final_title.replace('<![CDATA[', '').replace(']]>', '')

        a_soup = BeautifulSoup(raw_html, 'html.parser')
        
        imgs_html = ""
        for img in a_soup.find_all('img'):
            src = img.get('src')
            if src and '/res/' in src:
                full_img_url = urljoin(link, src)
                imgs_html += f'<figure><img src="{full_img_url}" style="max-width:100%;height:auto;"></figure><br>'

        content_div = a_soup.find(id="ozoom")
        content_html = str(content_div) if content_div else "<p>ï¼ˆå…§æ–‡æ“·å–å¤±æ•—ï¼‰</p>"
        full_content = f"{imgs_html}{content_html}".replace(']]>', ']]&gt;')

        return (i, final_title, link, full_content)
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±æ•—: {link} - {str(e)}")
        return (i, "æŠ“å–å¤±æ•—", link, f"<p>éŒ¯èª¤: {str(e)}</p>")

def start_multi_threaded_crawler(target_url, num_threads=8):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0"
    }

    print(f"ğŸ” é–‹å§‹è§£æç›®éŒ„: {target_url}")
    try:
        res = requests.get(target_url, headers=headers, timeout=15)
        if res.status_code != 200:
            print(f"âš ï¸ ç„¡æ³•å–å¾—ç¶²é  (HTTP {res.status_code})ï¼Œå¯èƒ½ä»Šæ—¥å ±ç´™å°šæœªæ›´æ–°ã€‚")
            return None

        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        links = []
        for a in soup.find_all('a', href=True):
            if 'content_' in a['href']:
                links.append(urljoin(target_url, a['href']))
        
        article_links = list(dict.fromkeys(links))
        total = len(article_links)
        
        if total == 0:
            print("âš ï¸ æ‰¾ä¸åˆ°ä»»ä½•æ–‡ç« é€£çµã€‚")
            return None

        print(f"ğŸš€ æ‰¾åˆ° {total} ç¯‡æ–‡ç« ï¼Œå•Ÿå‹• {num_threads} ç·šç¨‹è™•ç†ä¸­...")
        results = []
        
        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            future_to_url = {executor.submit(fetch_single_article, i, link, headers): i for i, link in enumerate(article_links)}
            
            completed_count = 0
            for future in as_completed(future_to_url):
                results.append(future.result())
                completed_count += 1
                if completed_count % 10 == 0 or completed_count == total:
                    print(f"â³ é€²åº¦: {completed_count}/{total}")

        # æŒ‰åŸå§‹é †åºæ’åº
        results.sort(key=lambda x: x[0])

        date_match = re.search(r'(\d{4}-\d{2}/\d{2})', target_url)
        date_str = date_match.group(1) if date_match else "Archive"
        
        if date_match:
            dt = datetime.datetime.strptime(date_str, "%Y-%m/%d")
            tz = datetime.timezone(datetime.timedelta(hours=8))
            pub_dt = dt.replace(tzinfo=tz, hour=8, minute=0)
        else:
            pub_dt = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=8)))
        
        rfc_date = email.utils.format_datetime(pub_dt)
        last_build_date = email.utils.format_datetime(datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=8))))

        # çµ„åˆ XML
        xml_parts = [
            '<?xml version="1.0" encoding="UTF-8"?>',
            '<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">',
            '  <channel>',
            f'    <title>æ¾³é–€æ—¥å ± - {date_str}</title>',
            f'    <link>{target_url}</link>',
            '    <description>æ¾³é–€æ—¥å ±ç•¶æ—¥æ–°èè‡ªå‹•æŠ“å–è¨‚é–±æº (GitHub Actions è‡ªå‹•ç”Ÿæˆ)</description>',
            '    <language>zh-hk</language>',
            f'    <pubDate>{rfc_date}</pubDate>',
            f'    <lastBuildDate>{last_build_date}</lastBuildDate>'
        ]

        for r in results:
            idx, title, link, content = r
            safe_title = saxutils.escape(title)
            safe_link = saxutils.escape(link)
            
            xml_parts.append('    <item>')
            xml_parts.append(f'      <title>{safe_title}</title>')
            xml_parts.append(f'      <link>{safe_link}</link>')
            xml_parts.append(f'      <guid isPermaLink="true">{safe_link}</guid>')
            xml_parts.append(f'      <pubDate>{rfc_date}</pubDate>')
            xml_parts.append(f'      <description><![CDATA[{content}]]></description>')
            xml_parts.append('    </item>')

        xml_parts.append('  </channel>')
        xml_parts.append('</rss>')

        print("âœ¨ RSS è½‰æ›å®Œæˆï¼")
        return "\n".join(xml_parts)

    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        return None

if __name__ == "__main__":
    # ç¢ºä¿å–å¾—çš„æ˜¯ UTC+8 (æ¾³é–€æ™‚é–“) çš„ä»Šå¤©æ—¥æœŸ
    tz = datetime.timezone(datetime.timedelta(hours=8))
    now = datetime.datetime.now(tz)
    formatted_date = now.strftime("%Y-%m/%d")
    
    today_url = f"https://www.macaodaily.com/html/{formatted_date}/node_1.htm"
    
    xml_content = start_multi_threaded_crawler(today_url, num_threads=8)
    
    if xml_content:
        # å°‡çµæœå¯«å…¥ rss.xml
        with open("rss.xml", "w", encoding="utf-8") as f:
            f.write(xml_content)
        print("âœ… æˆåŠŸç”Ÿæˆ rss.xml")
    else:
        print("âš ï¸ æŠ“å–ä¸­æ–·ï¼Œæœªç”Ÿæˆæ–°çš„ rss.xmlã€‚å¯èƒ½ä»Šæ—¥å ±ç´™å°šæœªå‡ºåˆŠã€‚")
        sys.exit(0) # ä»¥æ­£å¸¸ç‹€æ…‹é€€å‡ºï¼Œé¿å… Github Action å ±éŒ¯ç´…ç‡ˆ
